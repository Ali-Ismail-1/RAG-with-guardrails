# docker-compose.yaml
version: "3.9"
services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      # Choose your provider
      - llm_provider=${llm_provider:-ollama}
      - openai_model=${openai_model:-gpt-4o-mini}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ollama_model=${ollama_model:-llama3.8b}
      - ollama_base_url=${ollama_base_url:-http://host.docker.internal:11434}
      - embeddings_model=${embeddings_model:-sentence-transformers/all-MiniLM-L6-v2}
    ports:
      - "8000:8000"
    volumes:
      # Persist docs and chroma outside the container
      - ./data/docs:/app/data/docs
      - ./data/chroma:/app/data/chroma
    restart: unless-stopped

  ui:
    build:
      context: .
      dockerfile: ui.Dockerfile
    environment:
      - API_URL=http://api:8000/chat
    ports:
      - "8501:8501"
    depends_on:
      - api
    restart: unless-stopped
