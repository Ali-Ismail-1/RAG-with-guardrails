# RAG Chatbot (Compliant & Safe)

This project is a simple **Retrieval-Augmented Generation (RAG) chatbot** built with **FastAPI + LangChain + Chroma + Ollama/OpenAI**.  

It demonstrates how to build a chatbot that:
- Loads documents from `data/docs/`
- Creates embeddings and stores them in a **vector database** (Chroma)
- Retrieves the most relevant chunks for each query
- Passes them into an **LLM** with a custom prompt
- Maintains **chat history** across sessions
- Exposes an API with a `/chat` endpoint for interaction

---

## ğŸš€ Features

- **FastAPI API** with `/health` and `/chat` endpoints
- **Retriever + LLM orchestration** using LangChain
- **Chroma Vectorstore** with embeddings from HuggingFace
- **Session-based memory** (chat history tracked by `session_id`)
- **Pluggable LLM backends** (Ollama or OpenAI)
- **Guardrails** (filters, prompts, safe defaults)

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ api/ # API routes + request models
â”‚ â””â”€â”€ routes.py
â”œâ”€â”€ orchestration/ # RAG orchestration
â”‚ â””â”€â”€ chains.py
â”œâ”€â”€ memory/ # Vectorstore + chat history
â”‚ â”œâ”€â”€ vectorstore.py
â”‚ â””â”€â”€ history.py
â”œâ”€â”€ guardrails/ # Prompts + filters
â”‚ â”œâ”€â”€ prompts.py
â”‚ â””â”€â”€ filters.py
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ docs/ # Input documents (.md, .txt, .pdf)
â”‚ â””â”€â”€ chroma/ # Vectorstore persistence
â”œâ”€â”€ llm/ # LLM provider utils (Ollama / OpenAI)
â”‚ â””â”€â”€ providers.py
â”œâ”€â”€ main.py # FastAPI app entrypoint
â”œâ”€â”€ settings.py # Config (doc_dir, chroma_dir, models, etc.)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ Setup

### 1. Clone & Install
```bash
git clone <your-repo>
cd RAG-chatbot-compliant-safe
python -m venv venv
source venv/bin/activate   # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

---

## 2. Add Docs

Put .md, .txt, or .pdf files into:
```bash
data/docs/
```

## 3. Start Ollama (if using locally)
```powershell
ollama serve
ollama pull llama3:8b
ollama run llama3:8b
```

## 4. Run the API
```bash
uvicorn main:app --reload
```

---

## ğŸ“¡ Usage

### Healthcheck
```bash
curl http://localhost:8000/health
```

### Chat
```
$json = @"
{
  "session_id": "abc",
  "question": "What docs are in here?"
}
"@

curl -Uri "http://localhost:8000/chat" `
     -Method Post `
     -Headers @{ "Content-Type" = "application/json" } `
     -Body $json

```

Response:
```json
{
  "answer": "The provided context contains documentation in the form of Python tips..."
}
```

---

## ğŸ§© How It Works

**Vectorstore Build**
- Loads all docs in `data/docs/`
- Splits them into chunks
- Creates embeddings via HuggingFace
- Stores vectors in **Chroma** (`data/chroma/`)

**Retriever**
- Finds the top-k relevant chunks for a query

**Prompting**
- Wraps user question + retrieved context into a safe `BASE_PROMPT`

**LLM**
- Sends the prompt to **Ollama** (default) or **OpenAI**

**Memory**
- Tracks past conversation history by `session_id`

---

## ğŸ”§ Config

Edit `settings.py` (or use `.env`) to configure:

- `llm_provider` â†’ `"ollama"` or `"openai"`
- `ollama_model` â†’ `llama3:8b`, etc.
- `openai_model` â†’ `gpt-4o-mini`, etc.
- `doc_dir` â†’ where docs are loaded from
- `chroma_dir` â†’ where vectors are stored
- `embeddings_model` â†’ e.g. `"sentence-transformers/all-MiniLM-L6-v2"`

---

## ğŸ“Œ Roadmap

âœ… Current: Basic RAG pipeline (retriever + LLM + history)  
ğŸ”œ Add LangGraph orchestration for more complex flows  
ğŸ”œ Add UI (Streamlit or React) for a chat interface  
ğŸ”œ Add more guardrails (profanity filter, compliance checks)  
ğŸ”œ Add support for additional vector stores (Pinecone, Weaviate)  

---

## ğŸ›¡ï¸ Disclaimer
This project is for **educational use only**.  
Before deploying in production, add compliance checks, guardrails, monitoring, and security hardening.

---

## ğŸ“– References
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Ollama Models](https://ollama.ai/library)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

